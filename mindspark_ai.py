# -*- coding: utf-8 -*-
"""MindSpark AI

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1pYtal1g1i7yYwyRVCfXxBTRXW4jNqaPA

# MindSpark AI setup With Gemini
"""

!pip install -q -U google-generativeai

import pathlib
import textwrap

import google.generativeai as genai

from IPython.display import display
from IPython.display import Markdown


def to_markdown(text):
  text = text.replace('â€¢', '  *')
  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))

# Used to securely store your API key
from google.colab import userdata

# Or use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable.
GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')

genai.configure(api_key=GOOGLE_API_KEY)

"""# MindSpark List models"""

#Calling list models
for m in genai.list_models():
  if 'generateContent' in m.supported_generation_methods:
    print(m.name)

#Generate text inputs
model = genai.GenerativeModel('gemini-pro')

# Commented out IPython magic to ensure Python compatibility.
# #Response time
# %%time
# response = model.generate_content("Make a educational quiz with a 6th grade math textbook?")

#Markdown
to_markdown(response.text)

#Response Feedback
response.prompt_feedback

response.candidates

# Commented out IPython magic to ensure Python compatibility.
# %%time
# response = model.generate_content("Make a educational quiz with a 6th grade math textbook?", stream=True)

for chunk in response:
  print(chunk.text)
  print("_"*80)

response = model.generate_content("Ask 5 questions from this quiz?", stream=True)

response.prompt_feedback

try:
  response.text
except Exception as e:
  print(f'{type(e).__name__}: {e}')

"""# MindSpark Chat interaction"""

model = genai.GenerativeModel('gemini-pro')
chat = model.start_chat(history=[])
chat

response = chat.send_message("Make a game with 10 questions from a 6th grade math textbook.")
to_markdown(response.text)

chat.history

response = chat.send_message("Okay, how about a more detailed explanation to a middle schooler?", stream=True)

for chunk in response:
  print(chunk.text)
  print("_"*80)

for message in chat.history:
  display(to_markdown(f'**{message.role}**: {message.parts[0].text}'))

model = genai.GenerativeModel('gemini-pro')

messages = [
    {'role':'user',
     'parts': ["Make a game with 10 questions from a 6th grade math textbook."]}
]
response = model.generate_content(messages)

to_markdown(response.text)

messages.append({'role':'model',
                 'parts':[response.text]})

messages.append({'role':'user',
                 'parts':["Okay, how about a more detailed explanation to a middle school student?"]})

response = model.generate_content(messages)

to_markdown(response.text)

"""# Image Input and Generation"""

!curl -o image.jpg https://t0.gstatic.com/licensed-image?q=tbn:ANd9GcQ_Kevbk21QBRy-PgB4kQpS79brbmmEG7m3VOTShAn4PecDU5H5UxrJxE3Dw1JiaG17V88QIol19-3TM2wCHw

!curl -o image.png https://content.lessonplanet.com/resources/thumbnails/195766/large/njc0mjgylnbuzw.png?1414291796

!curl -o image.gif https://www.k5learning.com/sites/all/files/worksheets/math/grade-6-math-worksheet.gif

import PIL.Image

img = PIL.Image.open('image.png')
img

import PIL.Image

img = PIL.Image.open('image.jpg')
img

import PIL.Image

img = PIL.Image.open('image.gif')
img

model = genai.GenerativeModel('gemini-pro-vision')

response = model.generate_content(img)

to_markdown(response.text)

response = model.generate_content(["Make a educational quiz game from this image.", img], stream=True)
response.resolve()

to_markdown(response.text)

"""# Chatbox safety settings"""

response = model.generate_content('[Can you say a bad word?]')
response.candidates

response.prompt_feedback

response = model.generate_content('[Can you say a bad word?]',
                                  safety_settings={'HARASSMENT':'block_none'})
response.text

"""# Generative configurations"""

model.count_tokens("What is the meaning of life?")

model.count_tokens(chat.history)

model = genai.GenerativeModel('gemini-pro')
response = model.generate_content(
    'Tell me a story about a magic backpack.',
    generation_config=genai.types.GenerationConfig(
        # Only one candidate for now.
        candidate_count=1,
        stop_sequences=['x'],
        max_output_tokens=20,
        temperature=1.0)
)

text = response.text

if response.candidates[0].finish_reason.name == "MAX_TOKENS":
    text += '...'

to_markdown(text)

"""# Embeddings"""

result = genai.embed_content(
    model="models/embedding-001",
    content="What is the meaning of life?",
    task_type="retrieval_document",
    title="Embedding of single string")

# 1 input > 1 vector output
print(str(result['embedding'])[:50], '... TRIMMED]')

result = genai.embed_content(
    model="models/embedding-001",
    content=[
      'What is the meaning of life?',
      'How much wood would a woodchuck chuck?',
      'How does the brain work?'],
    task_type="retrieval_document",
    title="Embedding of list of strings")

# A list of inputs > A list of vectors output
for v in result['embedding']:
  print(str(v)[:50], '... TRIMMED ...')

response.candidates[0].content

result = genai.embed_content(
    model = 'models/embedding-001',
    content = response.candidates[0].content)

# 1 input > 1 vector output
print(str(result['embedding'])[:50], '... TRIMMED ...')

chat.history

result = genai.embed_content(
    model = 'models/embedding-001',
    content = chat.history)

# 1 input > 1 vector output
for i,v in enumerate(result['embedding']):
  print(str(v)[:50], '... TRIMMED...')

"""# Encoded messages"""

import google.ai.generativelanguage as glm

model = genai.GenerativeModel('gemini-pro-vision')
response = model.generate_content(
    glm.Content(
        parts = [
            glm.Part(text="Make a educational quiz game from this image."),
            glm.Part(
                inline_data=glm.Blob(
                    mime_type='image/png',
                    data=pathlib.Path('image.png').read_bytes()
                )
            ),
        ],
    ),
    stream=True)

response.resolve()

to_markdown(response.text[:100] + "... [TRIMMED] ...")
