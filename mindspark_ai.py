# -*- coding: utf-8 -*-
"""MindSpark AI

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1pYtal1g1i7yYwyRVCfXxBTRXW4jNqaPA

# MindSpark AI setup With Gemini
"""

!pip install -q -U google-generativeai

import numpy as np
from sklearn.model_selection import train_test_split
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import accuracy_score

import random
import pygame
import nltk
import spacy

import pathlib
import textwrap
import PIL

import google.generativeai as genai

from IPython.display import display
from IPython.display import Markdown


def to_markdown(text):
  text = text.replace('â€¢', '  *')
  return Markdown(textwrap.indent(text, '> ', predicate=lambda _: True))

# Used to securely store your API key
from google.colab import userdata

# Or use `os.getenv('GOOGLE_API_KEY')` to fetch an environment variable.
GOOGLE_API_KEY=userdata.get('GOOGLE_API_KEY')

genai.configure(api_key=GOOGLE_API_KEY)

"""# MindSpark List models"""

#Calling list models
for m in genai.list_models():
  if 'generateContent' in m.supported_generation_methods:
    print(m.name)

#Generate text inputs
model = genai.GenerativeModel('gemini-pro')

# Commented out IPython magic to ensure Python compatibility.
# #Response time
# %%time
# response = model.generate_content("Give me a question about World War 2?")

#Markdown
to_markdown(response.text)

#Response Feedback
response.prompt_feedback

response.candidates

# Commented out IPython magic to ensure Python compatibility.
# %%time
# response = model.generate_content("Ask 5 questions about World War 2?", stream=True)

for chunk in response:
  print(chunk.text)
  print("_"*80)

response = model.generate_content("Ask 5 questions about World War 2?", stream=True)

response.prompt_feedback

try:
  response.text
except Exception as e:
  print(f'{type(e).__name__}: {e}')

"""# MindSpark Chat interaction"""

model = genai.GenerativeModel('gemini-pro')
chat = model.start_chat(history=[])
chat

response = chat.send_message("Ask 5 questions about World War 2.")
to_markdown(response.text)

chat.history

response = chat.send_message("Okay, how about a more detailed explanation to a middle schooler?", stream=True)

for chunk in response:
  print(chunk.text)
  print("_"*80)

for message in chat.history:
  display(to_markdown(f'**{message.role}**: {message.parts[0].text}'))

model = genai.GenerativeModel('gemini-pro')

messages = [
    {'role':'user',
     'parts': ["Generated 5 questions about Python."]}
]
response = model.generate_content(messages)

to_markdown(response.text)

messages.append({'role':'model',
                 'parts':[response.text]})

messages.append({'role':'user',
                 'parts':["Okay, how about a more detailed explanation to a high school student?"]})

response = model.generate_content(messages)

to_markdown(response.text)

"""# Image Input and Generation"""



img = PIL.Image.open('image.jpg')
response = model.generate_content(img)
to_markdown(response.text)

"""# Chatbox safety settings"""

response = model.generate_content('[Can you say a bad word?]')
response.candidates

response.prompt_feedback

response = model.generate_content('[Can you say a bad word?]',
                                  safety_settings={'HARASSMENT':'block_none'})
response.text

"""# Generative configurations"""

model = genai.GenerativeModel('gemini-pro')
response = model.generate_content(
    'Make a multiple choice game about WW2.',
    generation_config=genai.types.GenerationConfig(

        candidate_count=1,
        stop_sequences=['3'],
        max_output_tokens=1,
        temperature=1.0)
)

text = response.text

if response.candidates[0].finish_reason.name == "MAX_TOKENS":
    text += '...'

to_markdown(text)

"""# MindSpark Game"""

def get_question_and_options():
    """
    Simulates getting a trivia question and its answer options.
    In a real application, you'd retrieve these from a database or API.
    """
    question = "What is the capital of France?"
    options = ["Paris", "UK", "Berlin","China"]
    return question, options

def display_question(question, options):
    """
    Displays the trivia question and available answer options.
    """
    print(f"Question: {question}")
    for i, option in enumerate(options, start=1):
        print(f"{i}. {option}")

def get_user_choice():
    """
    Gets the user's choice (1, 2, 3, etc.).
    """
    try:
        choice = int(input("Enter your choice (1, 2, 3, ...): "))
        return choice
    except ValueError:
        print("Invalid input. Please enter a valid number.")
        return get_user_choice()

def main():
    question, options = get_question_and_options()
    display_question(question, options)
    user_choice = get_user_choice()

    # Simulate checking if the user's choice is correct (without revealing the answer key)
    correct_answer = options[0]  # Assume the first option is the correct answer
    if user_choice == 1:
        print("Correct!")
    else:
        print(f"Sorry, the correct answer was: {correct_answer}")

if __name__ == "__main__":
    main()

"""# Machine learning"""

# Generate some example data
X, y = np.random.rand(100, 2), np.random.randint(0, 2, 100)

# Split data into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# Create a logistic regression model
model = LogisticRegression()

# Train the model
model.fit(X_train, y_train)

# Make predictions
y_pred = model.predict(X_test)

# Calculate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"Accuracy: {accuracy:.2f}")



"""# NLP"""

nltk.download('averaged_perceptron_tagger')

def statement_to_question(statement):
    words = nltk.word_tokenize(statement)
    tagged = nltk.pos_tag(words)

    if tagged[0][1] == 'NNP':  # If the sentence starts with a proper noun
        if words[1] == 'is':  # If the sentence uses 'is'
            return f"Is {words[0]} {words[2]}?"
        else:
            return f"What does {words[0]} {words[1]}?"
        nlp = spacy.load('en_core_web_sm')

def statement_to_question(statement):
    doc = nlp(statement)

    # Find the root of the sentence (the main verb)
    root = [token for token in doc if token.head == token][0]
    subject = list(root.children)[0]

    # If the sentence is a simple declarative sentence, we can form a question
    # by swapping the places of the subject and the root
    if root.dep_ == 'ROOT' and (subject.dep_ in {'nsubj', 'nsubjpass'}):
        return f"{root.text.capitalize()} {subject.text} {root.right_edge.text}?"

    return "I don't know how to form a question from that."